{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Experiment code with Langchain Ollama",
   "id": "9b13d4b21002d384"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the necessary libraries and set up the LLM",
   "id": "22f52c8629eb148a"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-23T23:28:27.416797Z",
     "start_time": "2025-07-23T23:28:27.413456Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T23:28:29.398629Z",
     "start_time": "2025-07-23T23:28:29.251883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from langfuse.langchain import CallbackHandler\n",
    "\n",
    "llm = ChatOllama(model=os.environ['OLLAMA_MODEL'], reasoning=True, callbacks=[CallbackHandler()])\n",
    "embeddings = OllamaEmbeddings(model=os.environ['OLLAMA_EMBEDDINGS'])\n"
   ],
   "id": "2a793aef0d54297e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Run some prompt for testing",
   "id": "9c29f613f939871f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T23:38:46.166374Z",
     "start_time": "2025-07-23T23:34:19.922440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', \"You are an {role}. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by\"),\n",
    "    ('human', \"{user_prompt}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | llm\n",
    "response = chain.invoke({\n",
    "    \"role\": \"AI assistant\",\n",
    "    \"user_prompt\": \"Suggest the best practices for parsing HTML documents with lxml\"\n",
    "})\n",
    "print(response)"
   ],
   "id": "ed62ac23fa7d8a31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='<think>\\nOkay, the user is asking for best practices for parsing HTML documents with lxml. Let me start by recalling what I know about lxml and HTML parsing. Lxml is a Python library that provides a powerful API for processing XML and HTML. It\\'s built on libxml2 and libxslt, which makes it pretty efficient.\\n\\nFirst, I should consider the main use cases. When parsing HTML, people often use lxml\\'s HTML parser, which is different from the XML parser. So maybe I should mention using the correct parser, like html.parser or the lxml built-in parser.\\n\\nThen, there\\'s the issue of handling malformed HTML. Since HTML is often not well-formed, the parser should be lenient. Maybe suggest using the parser that\\'s more forgiving, like the one that allows for some flexibility in syntax.\\n\\nNext, namespace handling. HTML doesn\\'t use namespaces, but sometimes people might mix XML namespaces. So advising to avoid namespace-aware parsing unless necessary could be important. Also, maybe mention that if namespaces are needed, they should be declared properly.\\n\\nAnother point is the use of XPath and CSS Selectors. lxml provides both, so suggesting which is more appropriate based on the task. For example, XPath is more powerful for complex queries, while CSS selectors are easier for simple cases.\\n\\nEfficient parsing is crucial. Maybe recommend using the parse method with a file or string, and using the correct parser. Also, avoiding unnecessary processing by selecting only the needed elements.\\n\\nHandling large documents: streaming or iterative parsing. Since lxml can handle large files, maybe suggest using a parser that allows for incremental processing, like the html parser with a stream.\\n\\nError handling is important too. Using try-except blocks to catch parsing errors, and maybe providing a fallback parser if the initial one fails.\\n\\nAlso, data extraction best practices. Using text() or .text_content() to get text, and avoiding common pitfalls like extra whitespace or line breaks.\\n\\nTesting with different HTML structures. Advising to test with various examples to ensure the parser handles different cases.\\n\\nPerformance considerations. Maybe suggest using the lxml\\'s built-in parser instead of others for better speed, and avoiding unnecessary element creation.\\n\\nSecurity aspects. Warning about potential risks of parsing untrusted HTML, like XSS, and suggesting sanitization if necessary.\\n\\nDocumentation and community resources. Pointing to the lxml documentation and examples for more details.\\n\\nWait, did I miss anything? Maybe the difference between using the built-in html parser vs others. Also, maybe the use of the etree.fromstring method for parsing from a string. Oh, and ensuring that the parser is set up correctly, like using the right parser object.\\n\\nAlso, when dealing with encodings, making sure the input is properly decoded. Maybe mention that when reading from a file, specifying the correct encoding is important to avoid errors.\\n\\nAnother thing: using the correct element methods, like using find() or findall() with XPath expressions. Also, the importance of closing the parser or file handles after use, though lxml might handle that automatically.\\n\\nI should structure these points into clear best practices, maybe numbered for clarity. Make sure to cover parser selection, error handling, efficiency, data extraction, security, and testing. Also, mention the use of XPath vs CSS selectors based on the task.\\n</think>\\n\\nParsing HTML documents with `lxml` (a powerful Python library for processing XML/HTML) requires careful handling to ensure robustness, efficiency, and correctness. Below are **best practices** for parsing HTML with `lxml`:\\n\\n---\\n\\n### **1. Use the Correct Parser**\\n- **Always use the HTML parser** for HTML documents, not the XML parser. HTML is often malformed, and the HTML parser is more forgiving:\\n  ```python\\n  from lxml import html\\n  tree = html.fromstring(html_content)\\n  ```\\n- If you need stricter validation, use `lxml.etree.XMLParser` with `recover=True` for error recovery:\\n  ```python\\n  from lxml import etree\\n  parser = etree.XMLParser(recover=True)\\n  tree = etree.fromstring(html_content, parser)\\n  ```\\n\\n---\\n\\n### **2. Handle Malformed HTML Gracefully**\\n- HTML is often inconsistent. Use the `recover=True` flag in the parser to handle missing tags, unclosed elements, or invalid syntax:\\n  ```python\\n  parser = etree.XMLParser(recover=True, recover_errors=True)\\n  ```\\n\\n---\\n\\n### **3. Avoid Namespace Conflicts**\\n- HTML does not use XML namespaces, but if you encounter namespace issues (e.g., with SVG or MathML), ensure you\\'re not using namespace-aware parsing unless necessary:\\n  ```python\\n  parser = etree.XMLParser(ns_clean=True, recover=True)\\n  ```\\n\\n---\\n\\n### **4. Use XPath or CSS Selectors Efficiently**\\n- **XPath** is more powerful for complex queries:\\n  ```python\\n  elements = tree.xpath(\\'//div[@class=\"content\"]/text()\\')\\n  ```\\n- **CSS Selectors** are simpler for basic tasks:\\n  ```python\\n  elements = tree.cssselect(\\'div.content\\')\\n  ```\\n- Avoid overusing `//` in XPath for large documents (it can be slow).\\n\\n---\\n\\n### **5. Parse Incrementally for Large Documents**\\n- For large HTML files, use **streaming parsing** with `lxml.html` or `etree`:\\n  ```python\\n  import lxml.html\\n  parser = lxml.html.HTMLParser(recover=True)\\n  tree = lxml.html.parse(\\'large.html\\', parser=parser)\\n  ```\\n\\n---\\n\\n### **6. Extract Text Carefully**\\n- Use `.text_content()` to extract all text within an element (removes extra whitespace):\\n  ```python\\n  text = element.text_content().strip()\\n  ```\\n- Avoid `.text` for nested elements; it only returns the text of the first child.\\n\\n---\\n\\n### **7. Handle Encoding and Input Correctly**\\n- Ensure input is properly decoded (e.g., UTF-8):\\n  ```python\\n  with open(\\'file.html\\', \\'r\\', encoding=\\'utf-8\\') as f:\\n      html_content = f.read()\\n  ```\\n\\n---\\n\\n### **8. Validate and Sanitize Input**\\n- **Do not parse untrusted HTML directly** due to security risks (e.g., XSS). Use libraries like `bleach` or `html-sanitizer` to sanitize content before parsing.\\n\\n---\\n\\n### **9. Optimize Performance**\\n- Avoid unnecessary element creation (e.g., use `find()` or `findall()` instead of iterating manually).\\n- Use `lxml`\\'s built-in methods for parsing and querying rather than manually manipulating strings.\\n\\n---\\n\\n### **10. Test with Real-World Examples**\\n- Test your parsing logic against malformed HTML, missing tags, and edge cases (e.g., nested tags, empty elements).\\n\\n---\\n\\n### **11. Use Context Managers for File Handling**\\n- Always use `with` statements when reading files to ensure proper cleanup:\\n  ```python\\n  with open(\\'file.html\\', \\'r\\') as f:\\n      tree = html.fromstring(f.read())\\n  ```\\n\\n---\\n\\n### **12. Leverage lxml\\'s Features**\\n- Use `lxml.html` for HTML-specific utilities (e.g., `html.tostring()` for serialization).\\n- For advanced tasks, combine `lxml` with `cssselect` or `parsel` for more flexible querying.\\n\\n---\\n\\n### **Example Workflow**\\n```python\\nfrom lxml import html\\n\\n# Parse HTML\\nhtml_content = \"<html><body><div class=\\'content\\'>Hello</div></body></html>\"\\ntree = html.fromstring(html_content)\\n\\n# Extract text using XPath\\ntext = tree.xpath(\\'//div[@class=\"content\"]/text()\\')[0]\\n\\n# Serialize back to HTML\\nserialized = html.tostring(tree, pretty_print=True).decode()\\nprint(serialized)\\n```\\n\\n---\\n\\nBy following these best practices, you can ensure your `lxml` parsing is robust, efficient, and secure. Always test with real-world HTML to handle edge cases.' additional_kwargs={} response_metadata={'model': 'qwen3', 'created_at': '2025-07-23T23:38:46.164226705Z', 'done': True, 'done_reason': 'stop', 'total_duration': 266228484241, 'load_duration': 107604572, 'prompt_eval_count': 58, 'prompt_eval_duration': 1772392953, 'eval_count': 1628, 'eval_duration': 264344321268, 'model_name': 'qwen3'} id='run--a771e39e-b289-473a-828f-502786d5a712-0' usage_metadata={'input_tokens': 58, 'output_tokens': 1628, 'total_tokens': 1686}\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
