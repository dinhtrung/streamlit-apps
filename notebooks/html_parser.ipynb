{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## HTML Layout structure parsing with crawl4ai",
   "id": "d4aa7c4022c8ed9e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Main implementation of the HTML Layout structure parsing with crawl4ai.",
   "id": "a8d22e4069aba703"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-24T00:58:19.415214Z",
     "start_time": "2025-07-24T00:58:19.389208Z"
    }
   },
   "source": [
    "import lxml\n",
    "from lxml.html.clean import Cleaner\n",
    "from collections import defaultdict\n",
    "from pydantic import BaseModel, ConfigDict\n",
    "from typing import List, Optional, Dict, Any\n",
    "\n",
    "# Define the Pydantic model for our analysis results\n",
    "class ElementAnalysis(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the analysis results for a single HTML element.\n",
    "    \"\"\"\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    text_count: int\n",
    "    text_ratio: float\n",
    "    tag_name: str\n",
    "    css_selector: str\n",
    "    xpath: str\n",
    "    _element: 'lxml.html.HtmlElement'\n",
    "\n",
    "    @property\n",
    "    def element(self) -> 'lxml.html.HtmlElement':\n",
    "        \"\"\"\n",
    "        Returns the original lxml.html.HtmlElement object.\n",
    "        \"\"\"\n",
    "        return self._element\n",
    "\n",
    "class HtmlLayoutAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyzes an HTML document to identify elements with the most text content.\n",
    "    Performs a cleaning step to remove non-text-related elements before analysis.\n",
    "    Returns a list of ElementAnalysis Pydantic models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, html_content: str):\n",
    "        \"\"\"\n",
    "        Initializes the analyzer with raw HTML content, cleaning it first.\n",
    "\n",
    "        Args:\n",
    "            html_content (str): The raw HTML content as a string.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            raw_tree = lxml.html.fromstring(html_content)\n",
    "            self.tree = self._clean_html(raw_tree)\n",
    "        except lxml.etree.ParserError as e:\n",
    "            raise ValueError(f\"Failed to parse HTML content: {e}\")\n",
    "        self.element_data = {}\n",
    "\n",
    "    def _clean_html(self, tree):\n",
    "        \"\"\"\n",
    "        Removes non-text-related elements and their content from the HTML tree.\n",
    "        This includes 'form', 'style', and 'script' tags.\n",
    "\n",
    "        Args:\n",
    "            tree: The lxml HTML tree to clean.\n",
    "\n",
    "        Returns:\n",
    "            The cleaned lxml HTML tree.\n",
    "        \"\"\"\n",
    "        cleaner = Cleaner(\n",
    "            page_structure=False,\n",
    "        )\n",
    "        return cleaner.clean_html(tree)\n",
    "\n",
    "    def _calculate_text_content(self, element) -> int:\n",
    "        \"\"\"\n",
    "        Recursively calculates the total text content (including descendants)\n",
    "        for a given HTML element.\n",
    "\n",
    "        Args:\n",
    "            element: An lxml.html element.\n",
    "\n",
    "        Returns:\n",
    "            int: The total length of text content within the element and its descendants.\n",
    "        \"\"\"\n",
    "        text_length = 0\n",
    "        if element.text:\n",
    "            text_length += len(element.text.strip())\n",
    "        for child in element:\n",
    "            text_length += self._calculate_text_content(child)\n",
    "            if child.tail:\n",
    "                text_length += len(child.tail.strip())\n",
    "        return text_length\n",
    "\n",
    "    def _get_element_css_selector(self, element) -> str:\n",
    "        \"\"\"\n",
    "        Generates a full CSS selector path for an lxml element.\n",
    "        \"\"\"\n",
    "        if element is None:\n",
    "            return \"\"\n",
    "\n",
    "        path_parts = []\n",
    "        current = element\n",
    "\n",
    "        while current is not None and current.tag != 'document':\n",
    "            if current.tag in [lxml.html.HtmlComment, lxml.html.HtmlProcessingInstruction, lxml.etree.Entity]:\n",
    "                current = current.getparent()\n",
    "                continue\n",
    "\n",
    "            tag_name = current.tag\n",
    "            path_segment = tag_name\n",
    "\n",
    "            element_id = current.get('id')\n",
    "            element_classes = current.get('class')\n",
    "\n",
    "            if element_id:\n",
    "                path_segment = f\"{tag_name}#{element_id}\"\n",
    "            elif element_classes:\n",
    "                classes = element_classes.strip().split()\n",
    "                if classes:\n",
    "                    class_selectors = \".\".join(classes)\n",
    "                    path_segment = f\"{tag_name}.{class_selectors}\"\n",
    "\n",
    "            parent = current.getparent()\n",
    "            if parent is not None:\n",
    "                index = 1\n",
    "                for sibling in parent:\n",
    "                    if sibling is current:\n",
    "                        break\n",
    "                    if sibling.tag == tag_name and \\\n",
    "                       sibling.tag not in [lxml.html.HtmlComment, lxml.html.HtmlProcessingInstruction, lxml.etree.Entity]:\n",
    "                        index += 1\n",
    "\n",
    "                siblings_with_same_tag = [s for s in parent\n",
    "                                          if s.tag == tag_name and\n",
    "                                          s.tag not in [lxml.html.HtmlComment, lxml.html.HtmlProcessingInstruction, lxml.etree.Entity]]\n",
    "\n",
    "                if len(siblings_with_same_tag) > 1 and not element_id:\n",
    "                    path_segment = f\"{path_segment}:nth-of-type({index})\"\n",
    "\n",
    "            path_parts.insert(0, path_segment)\n",
    "            current = current.getparent()\n",
    "\n",
    "        return \" > \".join(path_parts)\n",
    "\n",
    "    def _get_element_xpath(self, element) -> str:\n",
    "        \"\"\"\n",
    "        Generates a full, absolute XPath for an lxml element.\n",
    "        \"\"\"\n",
    "        if element is None:\n",
    "            return \"\"\n",
    "\n",
    "        path_parts = []\n",
    "        current = element\n",
    "\n",
    "        while current is not None and current.tag != 'document':\n",
    "            if current.tag in [lxml.html.HtmlComment, lxml.html.HtmlProcessingInstruction, lxml.etree.Entity]:\n",
    "                current = current.getparent()\n",
    "                continue\n",
    "\n",
    "            tag_name = current.tag\n",
    "            predicate = \"\"\n",
    "\n",
    "            element_id = current.get('id')\n",
    "            element_classes = current.get('class')\n",
    "\n",
    "            if element_id:\n",
    "                predicate = f\"[@id='{element_id}']\"\n",
    "            elif element_classes:\n",
    "                classes = element_classes.strip().split()\n",
    "                if classes:\n",
    "                    class_predicates = [f\"contains(concat(' ', @class, ' '), ' {cls} ')\" for cls in classes]\n",
    "                    predicate = f\"[{' and '.join(class_predicates)}]\"\n",
    "\n",
    "            if not predicate:\n",
    "                parent = current.getparent()\n",
    "                if parent is not None:\n",
    "                    index = 1\n",
    "                    for sibling in parent:\n",
    "                        if sibling is current:\n",
    "                            break\n",
    "                        if sibling.tag == tag_name and \\\n",
    "                           sibling.tag not in [lxml.html.HtmlComment, lxml.html.HtmlProcessingInstruction, lxml.etree.Entity]:\n",
    "                            index += 1\n",
    "\n",
    "                    if index > 1 or len([s for s in parent if s.tag == tag_name]) > 1:\n",
    "                        predicate = f\"[{index}]\"\n",
    "\n",
    "            path_parts.insert(0, f\"{tag_name}{predicate}\")\n",
    "            current = current.getparent()\n",
    "\n",
    "        if not path_parts:\n",
    "            return \"/\" + element.tag\n",
    "\n",
    "        full_path = \"/\" + \"/\".join(path_parts)\n",
    "        if full_path.startswith('/html') or full_path.startswith('/*'):\n",
    "             return full_path\n",
    "\n",
    "        return \"/\" + \"/\".join(path_parts)\n",
    "\n",
    "\n",
    "    def analyze_text_density(self, top_n: int = 3) -> List[ElementAnalysis]:\n",
    "        \"\"\"\n",
    "        Traverses the HTML tree, calculates text density for each element,\n",
    "        and returns the top N elements with the most text as Pydantic models.\n",
    "\n",
    "        Args:\n",
    "            top_n (int): The number of top elements to return. Defaults to 3.\n",
    "\n",
    "        Returns:\n",
    "            List[ElementAnalysis]: A list of Pydantic models, sorted by text_count.\n",
    "        \"\"\"\n",
    "        self.element_data = {}\n",
    "\n",
    "        body_element = self.tree.find('.//body')\n",
    "        if body_element is not None:\n",
    "            total_body_text_count = self._calculate_text_content(body_element)\n",
    "        else:\n",
    "            total_body_text_count = 0\n",
    "\n",
    "        for element in self.tree.xpath('//*'):\n",
    "            if isinstance(element.tag, str):\n",
    "                text_count = self._calculate_text_content(element)\n",
    "\n",
    "                text_ratio = 0.0\n",
    "                if total_body_text_count > 0:\n",
    "                    text_ratio = (text_count / total_body_text_count) * 100\n",
    "\n",
    "                css_selector = self._get_element_css_selector(element)\n",
    "                xpath = self._get_element_xpath(element)\n",
    "\n",
    "                if css_selector or xpath:\n",
    "                    self.element_data[css_selector] = {\n",
    "                        \"text_count\": text_count,\n",
    "                        \"text_ratio\": round(text_ratio, 2),\n",
    "                        \"tag_name\": element.tag,\n",
    "                        \"css_selector\": css_selector,\n",
    "                        \"xpath\": xpath,\n",
    "                        \"_element\": element\n",
    "                    }\n",
    "\n",
    "        sorted_elements = sorted(\n",
    "            self.element_data.values(),\n",
    "            key=lambda item: item[\"text_count\"],\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        result = []\n",
    "        for data in sorted_elements[:top_n]:\n",
    "            result.append(ElementAnalysis(**data))\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def determine_page_type(self) -> str:\n",
    "        \"\"\"\n",
    "        Heuristically determines if the page is a 'detail' page or a 'listing' page\n",
    "        based on the text distribution of the most text-dense elements.\n",
    "\n",
    "        Returns:\n",
    "            str: \"detail\" if the page has one highly dominant text block,\n",
    "                 \"listing\" if text is distributed more evenly, or \"unknown\".\n",
    "        \"\"\"\n",
    "        # Get the top 5 most text-dense elements to analyze their distribution\n",
    "        top_elements = self.analyze_text_density(top_n=5)\n",
    "\n",
    "        # Handle pages with very little content\n",
    "        if not top_elements or top_elements[0].text_count < 100:  # A minimum threshold\n",
    "            return \"unknown\"\n",
    "\n",
    "        # Heuristic 1: Is the top element's text ratio very high?\n",
    "        # A detail page often has a single element with > 50% of the body's text.\n",
    "        if top_elements[0].text_ratio > 50.0:\n",
    "            return \"detail\"\n",
    "\n",
    "        # Heuristic 2: How does the top element compare to the next one?\n",
    "        # On a listing page, the top few elements should have a similar amount of text.\n",
    "        # Check if the top element is not overwhelmingly larger than the second one.\n",
    "        if len(top_elements) > 1:\n",
    "            ratio_of_top_to_second = top_elements[0].text_count / top_elements[1].text_count\n",
    "\n",
    "            # If the top element is only slightly larger than the second, it's likely a listing.\n",
    "            # A value like < 2.0 (i.e., top element is less than 2x larger than the second)\n",
    "            if ratio_of_top_to_second < 2.0:\n",
    "                 return \"listing\"\n",
    "\n",
    "        # If neither of the above heuristics match, we return 'unknown'\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "\n",
    "def find_most_relevant_selectors(all_results: List[List[ElementAnalysis]], min_occurrence: int = 5) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Analyzes a list of lists of ElementAnalysis models to find the most relevant\n",
    "    CSS selectors based on average text_ratio and frequency.\n",
    "\n",
    "    Args:\n",
    "        all_results (List[List[ElementAnalysis]]): A list where each item is a list\n",
    "                                                   of ElementAnalysis models from one page.\n",
    "        min_occurrence (int): The minimum number of pages a selector must appear in\n",
    "                              to be considered relevant.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of dictionaries, each containing a selector, its\n",
    "                              average text_ratio, and its count, sorted by average ratio.\n",
    "    \"\"\"\n",
    "    selector_dict = defaultdict(lambda: {'ratios': [], 'count': 0, 'tag_name': '', 'example_xpath': ''})\n",
    "\n",
    "    for page_results in all_results:\n",
    "        # Keep track of which selectors we've seen on this page to avoid duplicates\n",
    "        seen_selectors_on_page = set()\n",
    "        for element_analysis in page_results:\n",
    "            selector = element_analysis.css_selector\n",
    "            if selector not in seen_selectors_on_page:\n",
    "                selector_dict[selector]['ratios'].append(element_analysis.text_ratio)\n",
    "                selector_dict[selector]['count'] += 1\n",
    "                selector_dict[selector]['tag_name'] = element_analysis.tag_name\n",
    "                selector_dict[selector]['example_xpath'] = element_analysis.xpath\n",
    "                seen_selectors_on_page.add(selector)\n",
    "\n",
    "    # Calculate average ratio and filter based on min_occurrence\n",
    "    aggregated_results = []\n",
    "    for selector, data in selector_dict.items():\n",
    "        if data['count'] >= min_occurrence:\n",
    "            average_ratio = sum(data['ratios']) / len(data['ratios'])\n",
    "            aggregated_results.append({\n",
    "                'css_selector': selector,\n",
    "                'tag_name': data['tag_name'],\n",
    "                'average_text_ratio': round(average_ratio, 2),\n",
    "                'occurrence_count': data['count'],\n",
    "                'example_xpath': data['example_xpath']\n",
    "            })\n",
    "\n",
    "    # Sort by average text ratio in descending order\n",
    "    aggregated_results.sort(key=lambda x: x['average_text_ratio'], reverse=True)\n",
    "\n",
    "    return aggregated_results\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T00:58:23.700500Z",
     "start_time": "2025-07-24T00:58:23.682546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "detail_page_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html><body>\n",
    "    <main id=\"main-content\">\n",
    "        <h1>Article Title</h1>\n",
    "        <article class=\"main-article\">\n",
    "            <p>This is the start of a very long article. It has many paragraphs\n",
    "            and a lot of descriptive text that makes up the bulk of the page's\n",
    "            content. This is the key content area.</p>\n",
    "            <p>More content here to make the page text-heavy.</p>\n",
    "            <p>Even more content to dominate the text distribution.</p>\n",
    "        </article>\n",
    "    </main>\n",
    "    <div class=\"sidebar\">\n",
    "        <p>A small amount of text here.</p>\n",
    "    </div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "listing_page_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html><body>\n",
    "    <h1>Search Results</h1>\n",
    "    <div class=\"results-list\">\n",
    "        <div class=\"result-item\">\n",
    "            <h2>Result 1</h2>\n",
    "            <p>Short snippet of text for result 1.</p>\n",
    "        </div>\n",
    "        <div class=\"result-item\">\n",
    "            <h2>Result 2</h2>\n",
    "            <p>Short snippet of text for result 2.</p>\n",
    "        </div>\n",
    "        <div class=\"result-item\">\n",
    "            <h2>Result 3</h2>\n",
    "            <p>Short snippet of text for result 3.</p>\n",
    "        </div>\n",
    "        <div class=\"footer\">\n",
    "            <p>Some text in the footer.</p>\n",
    "        </div>\n",
    "    </div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "analyzer_detail = HtmlLayoutAnalyzer(detail_page_html)\n",
    "page_type_detail = analyzer_detail.determine_page_type()\n",
    "print(f\"Detail Page HTML Type: {page_type_detail}\")\n",
    "\n",
    "analyzer_listing = HtmlLayoutAnalyzer(listing_page_html)\n",
    "page_type_listing = analyzer_listing.determine_page_type()\n",
    "print(f\"Listing Page HTML Type: {page_type_listing}\")\n",
    "\n",
    "# Example for an \"unknown\" page\n",
    "empty_page_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html><body>\n",
    "    <img src=\"logo.png\">\n",
    "    <a href=\"#\">Link</a>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "analyzer_empty = HtmlLayoutAnalyzer(empty_page_html)\n",
    "page_type_empty = analyzer_empty.determine_page_type()\n",
    "print(f\"Empty Page HTML Type: {page_type_empty}\")"
   ],
   "id": "8b05ac89e38f17eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detail Page HTML Type: detail\n",
      "Listing Page HTML Type: detail\n",
      "Empty Page HTML Type: unknown\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-24T00:57:57.645639Z",
     "start_time": "2025-07-24T00:57:57.625065Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's simulate a list of results from 3 different HTML samples\n",
    "# We'll use our analyzer to get the data for each sample.\n",
    "\n",
    "html_sample_1 = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html><body>\n",
    "    <main id=\"main-content\">\n",
    "        <section class=\"intro\">\n",
    "            <p>Text for the first sample, intro section.</p>\n",
    "            <p>More text here.</p>\n",
    "        </section>\n",
    "    </main>\n",
    "    <div class=\"sidebar\">\n",
    "        <p>Sidebar text, not very long.</p>\n",
    "    </div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "html_sample_2 = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html><body>\n",
    "    <main id=\"main-content\">\n",
    "        <section class=\"intro\">\n",
    "            <p>This is the text for the second sample. It's quite a bit longer.</p>\n",
    "            <p>This section is very text-dense and a key part of the page.</p>\n",
    "            <p>Another paragraph to make it even more content-rich.</p>\n",
    "        </section>\n",
    "    </main>\n",
    "    <div class=\"sidebar\">\n",
    "        <p>Short sidebar text.</p>\n",
    "    </div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "html_sample_3 = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html><body>\n",
    "    <article id=\"main-content\">\n",
    "        <section class=\"intro\">\n",
    "            <p>Sample 3 content, very similar to the others.</p>\n",
    "        </section>\n",
    "    </article>\n",
    "    <div class=\"footer\">\n",
    "        <p>Footer text.</p>\n",
    "    </div>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "\n",
    "# Process each sample using the analyzer\n",
    "analyzer_1 = HtmlLayoutAnalyzer(html_sample_1)\n",
    "results_1 = analyzer_1.analyze_text_density(top_n=5)\n",
    "\n",
    "analyzer_2 = HtmlLayoutAnalyzer(html_sample_2)\n",
    "results_2 = analyzer_2.analyze_text_density(top_n=5)\n",
    "\n",
    "analyzer_3 = HtmlLayoutAnalyzer(html_sample_3)\n",
    "results_3 = analyzer_3.analyze_text_density(top_n=5)\n",
    "\n",
    "# Create the list of lists of ElementAnalysis models\n",
    "all_page_results = [results_1, results_2, results_3]\n",
    "\n",
    "# Find the most relevant selectors with a minimum occurrence of 2 out of 3 samples\n",
    "relevant_selectors = find_most_relevant_selectors(all_page_results, min_occurrence=2)\n",
    "\n",
    "print(\"Most relevant CSS selectors based on text ratio across the dataset:\")\n",
    "print(\"-\" * 60)\n",
    "for selector_data in relevant_selectors:\n",
    "    print(f\"Selector: {selector_data['css_selector']}\")\n",
    "    print(f\"  Tag Name: {selector_data['tag_name']}\")\n",
    "    print(f\"  Average Text Ratio: {selector_data['average_text_ratio']}%\")\n",
    "    print(f\"  Appeared in {selector_data['occurrence_count']} out of {len(all_page_results)} samples\")\n",
    "    print(f\"  Example XPath: {selector_data['example_xpath']}\")\n",
    "    print(\"-\" * 60)"
   ],
   "id": "8fc3a916d2d99e8f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant CSS selectors based on text ratio across the dataset:\n",
      "------------------------------------------------------------\n",
      "Selector: html\n",
      "  Tag Name: html\n",
      "  Average Text Ratio: 100.0%\n",
      "  Appeared in 3 out of 3 samples\n",
      "  Example XPath: /html\n",
      "------------------------------------------------------------\n",
      "Selector: html > body\n",
      "  Tag Name: body\n",
      "  Average Text Ratio: 100.0%\n",
      "  Appeared in 3 out of 3 samples\n",
      "  Example XPath: /html/body\n",
      "------------------------------------------------------------\n",
      "Selector: html > body > section.intro\n",
      "  Tag Name: section\n",
      "  Average Text Ratio: 78.44%\n",
      "  Appeared in 2 out of 3 samples\n",
      "  Example XPath: /html/body/section[contains(concat(' ', @class, ' '), ' intro ')]\n",
      "------------------------------------------------------------\n",
      "Selector: html > body > section.intro > p:nth-of-type(1)\n",
      "  Tag Name: p\n",
      "  Average Text Ratio: 40.9%\n",
      "  Appeared in 2 out of 3 samples\n",
      "  Example XPath: /html/body/section[contains(concat(' ', @class, ' '), ' intro ')]/p[1]\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
