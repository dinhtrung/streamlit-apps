{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Website Analyzer\n",
    "\n",
    "- Detect if website is support multiple languages\n",
    "- Detect if bot detection is enabled to provide proper crawling configuration\n",
    "- See if text mode can be used to improve crawling speed\n",
    "- Check if a sitemap is available and if it is updated regularly\n",
    "\n",
    "### Test URLs\n",
    "\n",
    "- https://eur-lex.europa.eu/homepage.html\n",
    "-"
   ],
   "id": "266e112046c35333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T23:32:29.638815Z",
     "start_time": "2025-07-25T23:32:29.631774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_url = \"https://eur-lex.europa.eu/homepage.html\"\n",
    "target_file = \"eur-lex.europa.eu.json\""
   ],
   "id": "c453d7f399aee68a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Output Result",
   "id": "33bf6a9366fe2223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T23:45:11.432656Z",
     "start_time": "2025-07-25T23:45:11.419054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "class SiteMetadata(BaseModel):\n",
    "    home_page_url: Optional[str] = Field(default=None, description=\"The URL of the home page\"),\n",
    "    language_codes: Optional[List[str]] = Field(default=None, description=\"List of language codes supported by the website\"),\n",
    "    bot_detection_enabled: Optional[bool] = Field(default=None, description=\"Whether bot detection is enabled\"),\n",
    "    text_mode_enabled: Optional[bool] = Field(default=None, description=\"Whether text mode is enabled for crawling\"),\n",
    "    has_sitemap: Optional[bool] = Field(default=None, description=\"Indicate if a sitemap is available for URL discovery\"),"
   ],
   "id": "47309fc87e8e1102",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "target_site = SiteMetadata()",
   "id": "37073ef78f7d01ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep crawling example: Explore a website dynamically\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
    "from crawl4ai.models import CrawlResultContainer\n",
    "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
    "import json\n",
    "# Configure a 2-level deep crawl\n",
    "config = CrawlerRunConfig(\n",
    "    deep_crawl_strategy=BFSDeepCrawlStrategy(\n",
    "        max_depth=2,           # Crawl 2 levels deep\n",
    "        include_external=False, # Stay within domain\n",
    "        max_pages=50           # Limit for efficiency\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "async with AsyncWebCrawler() as crawler:\n",
    "    # Start crawling and follow links dynamically\n",
    "    results: CrawlResultContainer = await crawler.arun(target_url, config=config)\n",
    "    print(f\"Discovered and crawled {len(results)} pages\")\n",
    "    for result in results[:3]:\n",
    "        print(f\"Found: {result.url} at depth {result.metadata.get('depth', 0)}\")\n",
    "    # Save data for later reuse\n",
    "    with open(\"website_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps([ r.model_dump() for r in results]))\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T23:45:11.316375Z",
     "start_time": "2025-07-25T23:35:12.727517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# URL seeding example: Analyze all documentation\n",
    "from crawl4ai import AsyncUrlSeeder, SeedingConfig\n",
    "\n",
    "seeder = AsyncUrlSeeder()\n",
    "config = SeedingConfig(\n",
    "    source=\"sitemap\",\n",
    "    extract_head=True,\n",
    "    max_urls=100,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Get ALL documentation URLs instantly\n",
    "sitemap_urls = await seeder.urls(target_url, config)\n",
    "# 1000+ URLs discovered in seconds!\n",
    "if len(sitemap_urls) > 0:\n",
    "    print(f\"Discovered {len(sitemap_urls)} URLs from sitemap\")\n",
    "else:\n",
    "    print(\"No sitemap found. Fallback to use common crawl\")\n",
    "    sitemap_urls = await seeder.urls(target_url, config=SeedingConfig(\n",
    "    source=\"cc\",\n",
    "    extract_head=True,\n",
    "    max_urls=100,\n",
    "    verbose=True,\n",
    "))\n",
    "    print(f\"Discovered {len(sitemap_urls)} URLs from common crawl\")"
   ],
   "id": "d8d40df9846f7007",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sitemap found. Fallback to use common crawl\n",
      "Discovered 100 URLs from common crawl\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T00:51:24.837638Z",
     "start_time": "2025-07-26T00:51:24.832014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(target_file + \".meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(sitemap_urls, indent=4))"
   ],
   "id": "e905a32b206707b3",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T00:09:44.364876Z",
     "start_time": "2025-07-28T00:09:44.244352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "def detect_multi_language(sitemap_urls, threshold=0.1):\n",
    "    \"\"\"\n",
    "    Detect if a website is multi-language based on sitemap URLs.\n",
    "\n",
    "    :param sitemap_urls: List of URLs from the sitemap\n",
    "    :param threshold: Minimum proportion of URLs that should have language indicators\n",
    "    :return: Tuple (is_multi_language, detected_languages)\n",
    "    \"\"\"\n",
    "    language_patterns = [\n",
    "        r'/([a-z]{2})/', # matches /en/, /fr/, etc.\n",
    "        r'/([a-z]{2})-[a-z]{2}/', # matches /en-us/, /fr-ca/, etc.\n",
    "        r'\\.([a-z]{2})\\.' # matches .en., .fr., etc. in subdomains\n",
    "    ]\n",
    "\n",
    "    language_counts = Counter()\n",
    "    total_urls = len(sitemap_urls)\n",
    "\n",
    "    for url in sitemap_urls:\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        netloc = parsed_url.netloc\n",
    "\n",
    "        for pattern in language_patterns:\n",
    "            matches = re.findall(pattern, path) or re.findall(pattern, netloc)\n",
    "            if matches:\n",
    "                language_counts.update(matches)\n",
    "                break  # Count only one language indicator per URL\n",
    "\n",
    "    # Determine if it's multi-language\n",
    "    total_language_urls = sum(language_counts.values())\n",
    "    is_multi_language = (total_language_urls / total_urls) > threshold\n",
    "\n",
    "    # Get the list of detected languages\n",
    "    detected_languages = list(language_counts.keys())\n",
    "\n",
    "    return is_multi_language, detected_languages\n",
    "\n",
    "# Usage\n",
    "is_multi_language, detected_languages = detect_multi_language(sitemap_urls)\n",
    "\n",
    "print(f\"Is multi-language site: {is_multi_language}\")\n",
    "print(f\"Detected languages: {', '.join(detected_languages)}\")\n",
    "\n",
    "# Update the target_site object\n",
    "target_site.language_codes = detected_languages if is_multi_language else None"
   ],
   "id": "b40d42876e08d201",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sitemap_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 43\u001B[39m\n\u001B[32m     40\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m is_multi_language, detected_languages\n\u001B[32m     42\u001B[39m \u001B[38;5;66;03m# Usage\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m43\u001B[39m is_multi_language, detected_languages = detect_multi_language(\u001B[43msitemap_urls\u001B[49m)\n\u001B[32m     45\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mIs multi-language site: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mis_multi_language\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     46\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mDetected languages: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(detected_languages)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'sitemap_urls' is not defined"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
