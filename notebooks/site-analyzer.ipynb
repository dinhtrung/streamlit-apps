{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Website Analyzer\n",
    "\n",
    "- Detect if website is support multiple languages\n",
    "- Detect if bot detection is enabled to provide proper crawling configuration\n",
    "- See if text mode can be used to improve crawling speed\n",
    "- Check if a sitemap is available and if it is updated regularly\n",
    "\n",
    "### Test URLs\n",
    "\n",
    "- https://eur-lex.europa.eu/homepage.html\n",
    "-"
   ],
   "id": "266e112046c35333"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T23:32:29.638815Z",
     "start_time": "2025-07-25T23:32:29.631774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "target_url = \"https://eur-lex.europa.eu/homepage.html\"\n",
    "target_file = \"eur-lex.europa.eu.json\""
   ],
   "id": "c453d7f399aee68a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Output Result",
   "id": "33bf6a9366fe2223"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T23:45:11.432656Z",
     "start_time": "2025-07-25T23:45:11.419054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, List\n",
    "class SiteMetadata(BaseModel):\n",
    "    home_page_url: Optional[str] = Field(default=None, description=\"The URL of the home page\"),\n",
    "    language_codes: Optional[List[str]] = Field(default=None, description=\"List of language codes supported by the website\"),\n",
    "    bot_detection_enabled: Optional[bool] = Field(default=None, description=\"Whether bot detection is enabled\"),\n",
    "    text_mode_enabled: Optional[bool] = Field(default=None, description=\"Whether text mode is enabled for crawling\"),\n",
    "    has_sitemap: Optional[bool] = Field(default=None, description=\"Indicate if a sitemap is available for URL discovery\"),"
   ],
   "id": "47309fc87e8e1102",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "target_site = SiteMetadata()",
   "id": "37073ef78f7d01ea"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deep crawling example: Explore a website dynamically\n",
    "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig\n",
    "from crawl4ai.models import CrawlResultContainer\n",
    "from crawl4ai.deep_crawling import BFSDeepCrawlStrategy\n",
    "import json\n",
    "# Configure a 2-level deep crawl\n",
    "config = CrawlerRunConfig(\n",
    "    deep_crawl_strategy=BFSDeepCrawlStrategy(\n",
    "        max_depth=2,           # Crawl 2 levels deep\n",
    "        include_external=False, # Stay within domain\n",
    "        max_pages=50           # Limit for efficiency\n",
    "    ),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "async with AsyncWebCrawler() as crawler:\n",
    "    # Start crawling and follow links dynamically\n",
    "    results: CrawlResultContainer = await crawler.arun(target_url, config=config)\n",
    "    print(f\"Discovered and crawled {len(results)} pages\")\n",
    "    for result in results[:3]:\n",
    "        print(f\"Found: {result.url} at depth {result.metadata.get('depth', 0)}\")\n",
    "    # Save data for later reuse\n",
    "    with open(\"website_data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps([ r.model_dump() for r in results]))\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-25T23:45:11.316375Z",
     "start_time": "2025-07-25T23:35:12.727517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# URL seeding example: Analyze all documentation\n",
    "from crawl4ai import AsyncUrlSeeder, SeedingConfig\n",
    "\n",
    "seeder = AsyncUrlSeeder()\n",
    "config = SeedingConfig(\n",
    "    source=\"sitemap\",\n",
    "    extract_head=True,\n",
    "    max_urls=100,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Get ALL documentation URLs instantly\n",
    "sitemap_urls = await seeder.urls(target_url, config)\n",
    "# 1000+ URLs discovered in seconds!\n",
    "if len(sitemap_urls) > 0:\n",
    "    print(f\"Discovered {len(sitemap_urls)} URLs from sitemap\")\n",
    "else:\n",
    "    print(\"No sitemap found. Fallback to use common crawl\")\n",
    "    sitemap_urls = await seeder.urls(target_url, config=SeedingConfig(\n",
    "    source=\"cc\",\n",
    "    extract_head=True,\n",
    "    max_urls=100,\n",
    "    verbose=True,\n",
    "))\n",
    "    print(f\"Discovered {len(sitemap_urls)} URLs from common crawl\")"
   ],
   "id": "d8d40df9846f7007",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No sitemap found. Fallback to use common crawl\n",
      "Discovered 100 URLs from common crawl\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T00:51:24.837638Z",
     "start_time": "2025-07-26T00:51:24.832014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "with open(target_file + \".meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(json.dumps(sitemap_urls, indent=4))"
   ],
   "id": "e905a32b206707b3",
   "outputs": [],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
